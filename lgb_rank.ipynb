{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52580D361EAE4179AB021596E94AF4C1"
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录\n",
    "!ls /home/kesci/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F00D221CE3247479A366349CAEE391E"
   },
   "outputs": [],
   "source": [
    "# 查看个人持久化工作区文件\n",
    "!ls /home/kesci/work/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11418C7E60854EC78746DBB4A35D148C"
   },
   "outputs": [],
   "source": [
    "# 查看当前kernerl下的package\n",
    "!pip list --format=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6463E8752D343198C966AC65F8C4AC2"
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "283425EF870B4D2F8EDA564BE55DE8FC"
   },
   "outputs": [],
   "source": [
    "# lgb模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A1DDA69F53B4A68833EB90798B833B3"
   },
   "outputs": [],
   "source": [
    "# 存放读取特征并将所读取的所有特征拼接成一个dataframe的函数,\n",
    "# 注意在featurecol_h5中控制训练所用特征列\n",
    "# 注意在featurecol_map中控制读取特征特征列的位置\n",
    "\n",
    "# 数据转换函数工具\n",
    "\n",
    "# 计算qauc函数工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "0930EB8679FF4B0E8E3F4E3540822610"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime\n",
    "\n",
    "featurecol_h5={\n",
    "    'cross_feat':['query_in_title','query_title_pos'],\n",
    "    'query_pos_feat':['query_pos_1', 'query_pos_2', 'query_pos_3',\n",
    "                      'query_pos_4', 'query_pos_5', 'query_pos_6', \n",
    "                      'query_pos_7', 'query_pos_8', 'query_pos_9', \n",
    "                      'query_pos_10'\n",
    "                      ],\n",
    "    'sim_feat':['jaccard_q3_t3',  \n",
    "                'jaccard_q3_t5',  \n",
    "                'jaccard_q5_t5',   'levenshtein_q5_t5', \n",
    "                'jaccard_q5_t10',  'levenshtein_q5_t10', \n",
    "                'jaccard_q10_t10', 'levenshtein_q10_t10', \n",
    "                'jaccard_q15_t25', 'levenshtein_q15_t25',\n",
    "                'jaccard',         'levenshtein'],\n",
    "\n",
    "    'fuzz' :['token_sort_ratio','token_set_ratio',\n",
    "             'partial_ratio','partial_token_sort_ratio',\n",
    "             'partial_token_set_ratio', 'QRatio','WRatio'],\n",
    "    \n",
    "    'textpair':['total_unique_words','wc_ratio_unique',\n",
    "                'wc_diff_unique','token_set_diff','same_start'],\n",
    "    \n",
    "    'len_feat':[\"titlekw_num\", \"titlekw_querykw_diff\",\n",
    "                \"titlekw_querykw_sum\", \"titlekw_querykw_rate\"],\n",
    "    \n",
    "    \"nunique_feat\":[\"title_nunique_query\"],\n",
    "    \n",
    "    'title_score_count_feat':[\"title_score_count\",\"title_score_click_num\",\"title_click_rate\"],\n",
    "    'title_code_score_feat':[\"title_code_score\"],\n",
    "    'title_convert_feat':[\"title_code_convert\",\"title_code_label_count\"],\n",
    "    'count_feat':[\"title_count\"],\n",
    "    \n",
    "    'tag' :['tag'],\n",
    "    'tag_score_feat':[\"tag_score\"],\n",
    "    'tag_convert_feat':[\"tag_convert\",\"tag_label_count\"],\n",
    "    \n",
    "    \"match_feat\":['count_match', 'blockcount_match', 'proximity', 'maxMatchBlockLen',\n",
    "                  'q1_match_start', 'q1_match_end' ],\n",
    "    \n",
    "    'editdistance_relativepos':['editdistance','relative_pos'],\n",
    "    \n",
    "    \"BM25\":[\"BM25\"],\n",
    "    'sif_feat':[\"sif_cos\"],\n",
    "    'NN_SIM':['NN_SIM'],\n",
    "    \n",
    "    'sen_dis':['sent_cosine', 'sent_cityblock', \n",
    "                'sent_canberra', 'sent_euclidean', 'sent_minkowski',\n",
    "                'sent_braycurtis'],\n",
    "    \n",
    "    'sen_dis2':['skew_q','skew_t','kurtosis_q','kurtosis_t'],\n",
    "    \n",
    "}\n",
    "\n",
    "# 该函数功能是通过改变类型的方式降低pd中内存\n",
    "def reduce_mem_usage(D,verbose=True):\n",
    "    start_mem = D.memory_usage().sum() / 1024**2\n",
    "    for c, d in zip(D.columns, D.dtypes):\n",
    "        if d.kind == 'f':\n",
    "            D[c] = pd.to_numeric(D[c], downcast='float')\n",
    "        elif d.kind == 'i':\n",
    "            D[c] = pd.to_numeric(D[c], downcast='signed')\n",
    "    end_mem = D.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return D\n",
    "\n",
    "# def ReadData(datatype='train',nrows=40000000):\n",
    "def ReadData(datatype='train',start=0,nrows=100000000):\n",
    "    if datatype=='train':\n",
    "        id_feature='/home/kesci/input/bytedance/train_final.csv'\n",
    "        usecols=[0,4]\n",
    "        names=['query_id','label']\n",
    "        \n",
    "        print(\"正在读取：\",id_feature)\n",
    "        DataSet = pd.read_csv(id_feature,\n",
    "                              header=None,\n",
    "                              skiprows=start,\n",
    "                              nrows=nrows,\n",
    "                              usecols=usecols,\n",
    "                              names=names\n",
    "                              )\n",
    "        path_h5=\"/home/kesci/work/pre_3billion_data/train/\"\n",
    "    elif datatype=='test1':\n",
    "        id_feature='/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "        usecols=[0,2]\n",
    "        names=['query_id','query_title_id']\n",
    "        path_h5=\"/home/kesci/work/post_4kw_data/test1/\"\n",
    "        print(\"正在读取：\",id_feature)\n",
    "        DataSet = pd.read_csv(id_feature,\n",
    "                              header=None,\n",
    "                              skiprows=start,\n",
    "                              nrows=nrows,\n",
    "                              usecols=usecols,\n",
    "                              names=names\n",
    "                              )\n",
    "    elif datatype=='test2':\n",
    "        id_feature='/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "        usecols=[0,2]\n",
    "        names=['query_id','query_title_id']\n",
    "        path_h5=\"/home/kesci/work/post_4kw_data/test2/\"\n",
    "        print(\"正在读取：\",id_feature)\n",
    "        DataSet = pd.read_csv(id_feature,\n",
    "                              header=None,\n",
    "                              skiprows=start,\n",
    "                              nrows=nrows,\n",
    "                              usecols=usecols,\n",
    "                              names=names\n",
    "                              )\n",
    "\n",
    "    print(\"length:\",DataSet.__len__())\n",
    "    DataSet = reduce_mem_usage(DataSet, verbose=True)\n",
    "    \n",
    "    featuremap_h5={\n",
    "        'cross_feat':path_h5+f'cross_{datatype}_feat.h5',\n",
    "    \n",
    "        'query_pos_feat':path_h5+f'query_pos_{datatype}_feat.h5',\n",
    "        'title_pos_feat':path_h5+f'title_pos_{datatype}_feat.h5',\n",
    "    \n",
    "        'match_feat':path_h5+f'query_match_{datatype}_feat.h5',\n",
    "        'editDistance_feat':path_h5+f'editDistance_{datatype}_feat.h5',\n",
    "    \n",
    "        'sim_feat':path_h5+f'sim_{datatype}_feat.h5',\n",
    "        'tag_score_feat':path_h5+f'tag_score_10foldtime_{datatype}_feat.h5',\n",
    "        'title_score_count_feat':path_h5+f'title_score_count_{datatype}_feat.h5',\n",
    "        'title_code_score_feat':path_h5+f'title_code_score_10foldtime_{datatype}_feat.h5',\n",
    "        'title_convert_feat':path_h5+f'title_convert_{datatype}.h5',\n",
    "        'sif_feat':path_h5+f'sif_{datatype}_post_4kw.h5',\n",
    "        'len_feat':path_h5+f'len_{datatype}_feat.h5',\n",
    "        'count_feat':path_h5+f'count_feature_{datatype}.h5',\n",
    "        \"nunique_feat\":path_h5+f'nunique_feature_{datatype}.h5',\n",
    "    \n",
    "        'tag' :path_h5+f'tag_{datatype}.h5',\n",
    "        \"tag_convert_feat\":path_h5+f\"/tag_convert_{datatype}.h5\",\n",
    "        \"query_convert\":path_h5+f\"query_convert_{datatype}.h5\",\n",
    "    \n",
    "        \"M_cosine\":path_h5+f\"M_sim_{datatype}_feat.h5\",\n",
    "        \"M_tfidf_cosine\":path_h5+f\"M_tfidf_sim_{datatype}_feat.h5\",\n",
    "        \"BM25\":path_h5+f'BM25_{datatype}_feat.h5',\n",
    "        'NN_SIM':path_h5+f'nn_sim_feature.h5',\n",
    "    \n",
    "        'editdistance_relativepos':path_h5+f'editdistance_relativepos_{datatype}_feat.h5',\n",
    "        'fuzz' :path_h5+f\"fuzz_{datatype}_feat.h5\",\n",
    "        'textpair':path_h5+f\"textpair_{datatype}_feat.h5\",\n",
    "        \n",
    "        'sen_dis':path_h5+f\"sen_dis_{datatype}_200.h5\",\n",
    "        'sen_dis2':path_h5+f\"sen_dis2_{datatype}_200.h5\",\n",
    "    }\n",
    "\n",
    "    for featurefile in featurecol_h5:\n",
    "        print(\"正在读取：\",featuremap_h5[featurefile])\n",
    "        feature_set=pd.read_hdf(featuremap_h5[featurefile], key='data',start=start,stop=start+nrows).reset_index(drop=True)\n",
    "        print(\"length:\",feature_set.__len__())\n",
    "        # print(feature_set.head(1))\n",
    "        # feature_set=reduce_mem_usage(feature_set, verbose=True)\n",
    "        DataSet=pd.concat([DataSet,feature_set], axis=1)\n",
    "    \n",
    "    # DataSet = reduce_mem_usage(DataSet, verbose=True)\n",
    "    print(\"Data Read Finish!\")\n",
    "    return DataSet\n",
    "\n",
    "# *************************************** 数据转换 ************************************\n",
    "def getCalcfeat(FeatureData):\n",
    "    features=[c for c in FeatureData.columns]\n",
    "\n",
    "    if \"querykw_num\" in features and \"titlekw_num\" in features:\n",
    "        print(\"计算长度比率特征..\")\n",
    "        FeatureData[\"titlekw_querykw_rate\"]=FeatureData[\"titlekw_num\"]/FeatureData[\"querykw_num\"]\n",
    "        FeatureData[\"titlekw_querykw_diff\"]=FeatureData[\"titlekw_num\"]-FeatureData[\"querykw_num\"]\n",
    "        FeatureData[\"titlekw_querykw_sum\"]=FeatureData[\"titlekw_num\"]+FeatureData[\"querykw_num\"]\n",
    "    \n",
    "    if \"title_score_count\" in features and \"title_score_click_num\" in features:\n",
    "        print(\"计算title点击率特征..\")\n",
    "        FeatureData[\"title_click_rate\"]=0\n",
    "        FeatureData.loc[FeatureData.title_score_count!=0,\"title_click_rate\"]= \\\n",
    "            FeatureData.loc[FeatureData.title_score_count!=0,\"title_score_click_num\"]/ \\\n",
    "            FeatureData.loc[FeatureData.title_score_count!=0,\"title_score_count\"]\n",
    "\n",
    "    FeatureData = reduce_mem_usage(FeatureData, verbose=True)\n",
    "    print(\"calc feat done\")\n",
    "    return FeatureData\n",
    "\n",
    "def preDataXGBoost(FeatureData,hasLabel=True,datatype=None):\n",
    "    # 该函数的功能是进行是将pandas数据转换为numpy.array\n",
    "    FeatureData=getCalcfeat(FeatureData)\n",
    "    if hasLabel:\n",
    "        if datatype is not None:\n",
    "            group_path = f'/home/kesci/work/pre_3billion_data/groups_{datatype}.npy'\n",
    "            group=np.load(group_path)\n",
    "            target=FeatureData.label.get_values()\n",
    "        else:\n",
    "            print(\"error! Please set datatype...\")\n",
    "    cols=[]\n",
    "    for featurefile in featurecol_h5:\n",
    "        cols=cols+featurecol_h5[featurefile]\n",
    "    \n",
    "    featuredata = FeatureData[cols].get_values()\n",
    "\n",
    "    if hasLabel:\n",
    "        return group,featuredata,target\n",
    "    else:\n",
    "        return featuredata\n",
    "\n",
    "# *************************************** 计算auc源码 ************************************\n",
    "def calAUC(labels,prob):\n",
    "    f = list(zip(prob,labels))\n",
    "    rank = [values2 for values1,values2 in sorted(f,key=lambda x:x[0])]\n",
    "    rankList = [i+1 for i in range(len(rank)) if rank[i]==1]\n",
    "    posNum = 0\n",
    "    negNum = 0\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i]==1):\n",
    "            posNum+=1\n",
    "        else:\n",
    "            negNum+=1\n",
    "    auc = (sum(rankList)- (posNum*(posNum+1))/2)/(posNum*negNum)\n",
    "    return auc\n",
    "\n",
    "# 计算各个组的qauc值\n",
    "def sumAUC(mycombinedata):\n",
    "    grouplist=mycombinedata[0]\n",
    "    y_true=mycombinedata[1]\n",
    "    y_pred=mycombinedata[2]\n",
    "\n",
    "    if len(y_true)!=sum(grouplist):\n",
    "        print(\"评分函数中len(y_true)!=sum(group)\")\n",
    "        return\n",
    "    start=0\n",
    "    sum_AUC=0\n",
    "    for group in grouplist:\n",
    "        if 0 in y_true[start:start+group] and 1 in y_true[start:start+group]:\n",
    "            roc_auc = calAUC(y_true[start:start+group],y_pred[start:start+group])\n",
    "            # roc_auc=auc(fpr,tpr) ###计算auc的值\n",
    "        else:\n",
    "            roc_auc=0.5\n",
    "        start=start+group\n",
    "        sum_AUC=sum_AUC+roc_auc\n",
    "    return sum_AUC\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def qAUC(y_true,y_pred,group):\n",
    "    groupnum=16\n",
    "    import math\n",
    "    group_len=math.ceil(len(group)/groupnum)\n",
    "    groups=[group[i*group_len:(i+1)*group_len] for i in range(groupnum)]\n",
    "    mycombines=[]\n",
    "\n",
    "    start=0\n",
    "    for agroup in groups:\n",
    "        mycombinedata=[]\n",
    "        mycombinedata.append(agroup)\n",
    "        mycombinedata.append(y_true[start:start+sum(agroup)])\n",
    "        mycombinedata.append(y_pred[start:start+sum(agroup)])\n",
    "        start=start+sum(agroup)\n",
    "        mycombines.append(mycombinedata)\n",
    "\n",
    "    sum_AUC=Parallel(n_jobs=groupnum)(delayed(sumAUC)(mycombinedata) for mycombinedata in mycombines)\n",
    "\n",
    "    return \"qAUC\",sum(sum_AUC)/len(group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "2E3F25BBB7A94EBF87133A876EB1A9E8"
   },
   "outputs": [],
   "source": [
    "# 读取数据,并将数据划分数据集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "06A6E39A4D3D4DDE9303A742EF87A15C"
   },
   "outputs": [],
   "source": [
    "#  读取处理成特征的数据集（只使用了后 1500000 query_id 的数据）\n",
    "train_query_title = ReadData(datatype='train',nrows=100000000)\n",
    "print(\"数据读取完毕\")\n",
    "\n",
    "train_query_title.title_code_score=train_query_title.title_code_score.fillna(0)\n",
    "train_query_title.tag_score=train_query_title.tag_score.fillna(0)\n",
    "\n",
    "# 将前500w个query_id 作为验证集，后350w数据集作为训练集\n",
    "\n",
    "queryids=list(range(1,train_query_title.query_id.max()))\n",
    "val_queryids = queryids[:1000000]\n",
    "train_queryids = queryids[1000000:]\n",
    "\n",
    "val_query_title=train_query_title.loc[train_query_title.query_id.isin(val_queryids)]\n",
    "print(\"val len:\",val_query_title.__len__())\n",
    "train_query_title=train_query_title.loc[train_query_title.query_id.isin(train_queryids)]\n",
    "print(\"train len:\",train_query_title.__len__())\n",
    "\n",
    "del queryids,train_queryids,val_queryids\n",
    "gc.collect()\n",
    "\n",
    "print(\"测试集验证集划分完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "B15E19F4FB4D4A3BBD52C0A3331420FA"
   },
   "outputs": [],
   "source": [
    "# 对训练集按照queryid分组放入lgb.Dataset()中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "5E2D4C35C1FB46E28C1A9D6718EF0495"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "Train_flag=True\n",
    "Valdation_Flag=True\n",
    "import lightgbm as lgb\n",
    "\n",
    "if Train_flag:\n",
    "    # dgroup,dtrain,dtarget = preDataXGBoost(train_query_title,hasLabel=True,datatype=\"train\")\n",
    "    datatype=\"train\"\n",
    "    group_path = f'/home/kesci/work/pre_3billion_data/groups_{datatype}.npy'\n",
    "    train_data = lgb.Dataset(data=preDataXGBoost(train_query_title,hasLabel=False), \n",
    "                             label=train_query_title.label.get_values(),\n",
    "                             group=np.load(group_path))\n",
    "    del train_query_title\n",
    "    gc.collect()\n",
    "    print(\"train OK\")\n",
    "\n",
    "if Valdation_Flag:\n",
    "    # dvalgroup,dval,dvaltarget = preDataXGBoost(val_query_title,hasLabel=True,datatype=\"validation\")\n",
    "    datatype=\"validation\"\n",
    "    group_path = f'/home/kesci/work/pre_3billion_data/groups_{datatype}.npy'\n",
    "    \n",
    "    val_data = lgb.Dataset(data=preDataXGBoost(val_query_title,hasLabel=False), \n",
    "                           label=val_query_title.label.get_values(), \n",
    "                           group=np.load(group_path),\n",
    "                           reference=train_data)\n",
    "    \n",
    "    del val_query_title\n",
    "    gc.collect()\n",
    "    print(\"test OK\")\n",
    "print(\"数据转换完毕\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96E72DC380C948A3973AE27BEF07D241"
   },
   "outputs": [],
   "source": [
    "# lgb模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "B27A6A544B96463F87F86F7025FE9997"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "lgb_rank_params = {    \n",
    "    'boosting_type' : 'gbdt', \n",
    "    'objective' : 'lambdarank',\n",
    "    'metric': 'map',\n",
    "    'random_state' : 2019,\n",
    "    'n_jobs' : 13,\n",
    "    'num_leaves' : 195,\n",
    "    'max_depth' : 12,\n",
    "    'learning_rate':0.05,\n",
    "    'max_bin':200,\n",
    "    'subsample_for_bin':200000,\n",
    "    'min_split_gain':0.0,\n",
    "    'min_child_weight':0.001,\n",
    "    'min_child_samples':20,\n",
    "    'subsample':0.8,\n",
    "    'subsample_freq':1,\n",
    "    'colsample_bytree':0.8,\n",
    "    'reg_alpha':3.0,\n",
    "    'reg_lambda':2.0,\n",
    "    \n",
    "    # 'device':'gpu',\n",
    "    # 'gpu_platform_id':0,\n",
    "    # 'gpu_device_id':0\n",
    "}\n",
    "valdation_flag = True\n",
    "\n",
    "if valdation_flag:\n",
    "    print(\"Train 开始\")\n",
    "    start=time.time()\n",
    "    rankModel = lgb.train(lgb_rank_params, train_data, num_boost_round=3000,\\\n",
    "        valid_sets=[train_data,val_data],\n",
    "        verbose_eval=1,\n",
    "        early_stopping_rounds=300,\n",
    "        )\n",
    "    print(\"训练完成，训练时间为：\",time.time()-start)\n",
    "    \n",
    "    rankModel.save_model(\"/home/kesci/work/LGBModel/LGBRank_pre1e_807_final_baseline_3000_1.model\")\n",
    "    print(\"save model finish.\")\n",
    "    \n",
    "    start=time.time()\n",
    "    test_pred=rankModel.predict(dval)\n",
    "    print(\"预测完成，预测时间为：\",time.time()-start)\n",
    "    \n",
    "    start=time.time()\n",
    "    score_cur=qAUC(dvaltarget,test_pred,dvalgroup)\n",
    "    print(\"计算qAUC完成，计算时间为：\",time.time()-start)\n",
    "    print(\"qAUC值为：\",score_cur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EE848A045D8F463ABFD077CA4AAEE25F"
   },
   "outputs": [],
   "source": [
    "# 模型test1集预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "0FDC31B2C1BE4AF08FF0173CA4AC3B6D"
   },
   "outputs": [],
   "source": [
    "print(\"begin\")\n",
    "\n",
    "test_query_title = ReadData(datatype='test1')\n",
    "test_query_title.title_code_score=test_query_title.title_code_score.fillna(0)\n",
    "test_query_title.tag_score=test_query_title.tag_score.fillna(0)\n",
    "print(\"数据读取完毕\")\n",
    "\n",
    "dtest = preDataXGBoost(test_query_title,hasLabel=False)\n",
    "print(\"dtest OK\")\n",
    "\n",
    "from lightgbm import Dataset,Booster\n",
    "rankModel=Booster(model_file=\"/home/kesci/work/LGBModel/LGBRank_pre1e_807_final_baseline_3000_1.model\")\n",
    "\n",
    "test_pred=rankModel.predict(dtest)\n",
    "print(\"test_pred OK\")\n",
    "\n",
    "submission=test_query_title[[\"query_id\",\"query_title_id\"]]\n",
    "submission[\"prediction\"]=pd.DataFrame(test_pred)\n",
    "\n",
    "SaveFile=\"/home/kesci/work/Submission/LGBRank_pre1e_807_final_baseline_3000_1.csv\"\n",
    "submission.to_csv(path_or_buf=SaveFile,header=False,index=0,encoding=\"utf-8\")\n",
    "print(\"submission OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
