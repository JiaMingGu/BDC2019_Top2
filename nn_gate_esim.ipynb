{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "66EFEB3A89ED4576835E493140F2E34E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A02EF848AACD44838A8C3DF06175F37F"
   },
   "outputs": [],
   "source": [
    "# 以下代码是保存最后1e的数据到工作区，因为训练集数据量过大，直接通过跳行的方式读取会很慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "A20A7CADD1524E488882B53F1BE6F93D"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "chunksize = 300000000\n",
    "\n",
    "data_reader = pd.read_csv('', header=None, usecols=[0, 1, 3, 4], names=['query_id', 'query', 'title', 'label'],\n",
    "                          chunksize=chunksize)\n",
    "\n",
    "post_10kw_data = None\n",
    "\n",
    "for index, data in enumerate(data_reader):\n",
    "    if index == 3:\n",
    "        post_10kw_data = data\n",
    "    else:\n",
    "        del data\n",
    "        gc.collect()\n",
    "\n",
    "# 单独保存后1e的数据 用于训练词向量 训练模型等\n",
    "post_10kw_data.to_csv('/home/kesci/work/word2vec/post_10kw.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92C5DB03E0AC498DAF216222C1BF2897"
   },
   "outputs": [],
   "source": [
    "# final word2vec 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true,
    "id": "6370C29FA3564F488BB4664BE3891E77",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''用于保存模型, 打印损失函数等等'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = 0\n",
    "        self.best_loss = 999999999.9\n",
    "        self.since = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "        cum_loss = model.get_latest_training_loss()  # 返回的是从第一个epoch累计的\n",
    "        epoch_loss = cum_loss - self.pre_loss\n",
    "        time_taken = time.time() - self.since\n",
    "        print(\"Epoch %d, loss: %.2f, time: %dmin %ds\" %\n",
    "              (self.epoch, epoch_loss, time_taken // 60, time_taken % 60))\n",
    "        if self.best_loss > epoch_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            print(\"Better model. Best loss: %.2f\" % self.best_loss)\n",
    "            model.save('./word2vec/word2vec_100.model')\n",
    "            model.wv.save_word2vec_format('./word2vec/word2vec_100.bin', binary=True)\n",
    "            print(\"Model %s save done!\" % './word2vec/word2vec_100.model')\n",
    "\n",
    "        self.pre_loss = cum_loss\n",
    "        self.since = time.time()\n",
    "\n",
    "\n",
    "def log(log: str):\n",
    "    print(log)\n",
    "\n",
    "\n",
    "def time_log(time_elapsed):\n",
    "    print('complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))  # 打印时间\n",
    "\n",
    "\n",
    "def log_event(event: str):\n",
    "    log(event)\n",
    "\n",
    "\n",
    "def get_sentence(train_path_1: str, train_path2: str, test_2kw_path: str, test_1e_path: str, chunk_size: int):\n",
    "    for index, data in enumerate(\n",
    "            pd.read_csv(train_path_1, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'],\n",
    "                        nrows=200000000)):\n",
    "\n",
    "        print(f'train 1 path = {train_path_1} index = {index}')\n",
    "        query_df = data['query'].drop_duplicates()\n",
    "\n",
    "        query_list = query_df.values.tolist()\n",
    "        for item in query_list:\n",
    "            yield item\n",
    "\n",
    "        title_list = data['title'].values.tolist()\n",
    "        for item in title_list:\n",
    "            yield item\n",
    "        del query_list, title_list, query_df, data\n",
    "        gc.collect()\n",
    "\n",
    "    for index, data in enumerate(\n",
    "            pd.read_csv(train_path2, chunksize=chunk_size)):\n",
    "\n",
    "        print(f'train 2 path = {train_path2} index = {index}')\n",
    "        query_df = data['query'].drop_duplicates()\n",
    "\n",
    "        query_list = query_df.values.tolist()\n",
    "        for item in query_list:\n",
    "            yield item\n",
    "\n",
    "        title_list = data['title'].values.tolist()\n",
    "        for item in title_list:\n",
    "            yield item\n",
    "\n",
    "        del query_list, title_list, query_df, data\n",
    "        gc.collect()\n",
    "\n",
    "    for index, data in enumerate(\n",
    "            pd.read_csv(test_2kw_path, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'])):\n",
    "        print(f'test path = {test_2kw_path} index = {index}')\n",
    "\n",
    "        query_df = data['query'].drop_duplicates()\n",
    "\n",
    "        query_list = query_df.values.tolist()\n",
    "        for item in query_list:\n",
    "            yield item\n",
    "\n",
    "        title_list = data['title'].values.tolist()\n",
    "        for item in title_list:\n",
    "            yield item\n",
    "\n",
    "        del query_list, title_list, query_df, data\n",
    "        gc.collect()\n",
    "\n",
    "    for index, data in enumerate(\n",
    "            pd.read_csv(test_1e_path, chunksize=chunk_size, header=None, usecols=[1, 3], names=['query', 'title'])):\n",
    "        print(f'path = {test_1e_path} index = {index}')\n",
    "        query_df = data['query'].drop_duplicates()\n",
    "\n",
    "        query_list = query_df.values.tolist()\n",
    "        for item in query_list:\n",
    "            yield item\n",
    "\n",
    "        title_list = data['title'].values.tolist()\n",
    "        for item in title_list:\n",
    "            yield item\n",
    "\n",
    "        del query_list, title_list, query_df, data\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, train_path_1: str, train_path2: str, test_2kw_path: str, test_1e_path: str, chunk_size: int):\n",
    "        self.train_path_1 = train_path_1\n",
    "        self.train_path2 = train_path2\n",
    "        self.test_2kw_path = test_2kw_path\n",
    "        self.test_1e_path = test_1e_path\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sentence in get_sentence(self.train_path_1, self.train_path2, self.test_2kw_path, self.test_1e_path,\n",
    "                                     self.chunk_size):\n",
    "            seg_list = sentence.split()\n",
    "            yield seg_list\n",
    "\n",
    "\n",
    "sentences = Sentence('/home/kesci/input/bytedance/train_final.csv', '/home/kesci/work/word2vec/post_10kw.csv',\n",
    "                     '/home/kesci/input/bytedance/test_final_part1.csv',\n",
    "                     '/home/kesci/input/bytedance/bytedance_contest.final_2.csv',\n",
    "                     chunk_size=5000000)\n",
    "\n",
    "word2vec_dim = 100\n",
    "\n",
    "model = Word2Vec(size=word2vec_dim, window=5, sg=1, min_count=1, workers=4)\n",
    "\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=10, compute_loss=True, report_delay=5 * 60,\n",
    "            callbacks=[EpochSaver()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "A6ECA8EC6B95498EBD0CAFD38A76DC63"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "words_dict = {}\n",
    "\n",
    "\n",
    "text_data1 = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[1, 3],\n",
    "                         names=['query', 'title'], nrows=200000000)\n",
    "\n",
    "\n",
    "query_list = text_data1['query'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(query_list):\n",
    "    query = item.split()\n",
    "    for word in query:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del query_list\n",
    "gc.collect()\n",
    "\n",
    "title_list = text_data1['title'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(title_list):\n",
    "    title = item.split()\n",
    "    for word in title:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del title_list\n",
    "\n",
    "del text_data1\n",
    "gc.collect()\n",
    "\n",
    "text_data2 = pd.read_csv('/home/kesci/work/word2vec/post_10kw.csv')\n",
    "\n",
    "\n",
    "query_list = text_data2['query'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(query_list):\n",
    "    query = item.split()\n",
    "    for word in query:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del query_list\n",
    "gc.collect()\n",
    "\n",
    "title_list = text_data2['title'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(title_list):\n",
    "    title = item.split()\n",
    "    for word in title:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del title_list\n",
    "del text_data2\n",
    "gc.collect()\n",
    "\n",
    "test = pd.read_csv('/home/kesci/input/bytedance/test_final_part1.csv', usecols=[1, 3],\n",
    "                   names=['query', 'title'])\n",
    "\n",
    "\n",
    "query_list = test['query'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(query_list):\n",
    "    query = item.split()\n",
    "    for word in query:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del query_list\n",
    "gc.collect()\n",
    "\n",
    "title_list = test['title'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(title_list):\n",
    "    title = item.split()\n",
    "    for word in title:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del title_list\n",
    "del test\n",
    "gc.collect()\n",
    "\n",
    "test2 = pd.read_csv('/home/kesci/input/bytedance/bytedance_contest.final_2.csv', usecols=[1, 3],\n",
    "                    names=['query', 'title'])\n",
    "\n",
    "\n",
    "query_list = test2['query'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(query_list):\n",
    "    query = item.split()\n",
    "    for word in query:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del query_list\n",
    "gc.collect()\n",
    "\n",
    "title_list = test2['title'].drop_duplicates().values.tolist()\n",
    "\n",
    "for item in tqdm(title_list):\n",
    "    title = item.split()\n",
    "    for word in title:\n",
    "        words_dict[word] = words_dict.get(word, 0) + 1\n",
    "\n",
    "del title_list\n",
    "del test2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "B509EE1732A14AEDB2B2D9227B8A615A"
   },
   "outputs": [],
   "source": [
    "min_count = 1\n",
    "\n",
    "print(f'words_dict len = {len(words_dict)}')\n",
    "words = {i: j for i, j in list(words_dict.items()) if j >= min_count}\n",
    "id2words = {i + 2: j for i, j in enumerate(words)}  # padding: 0, unk: 1\n",
    "words2id = {j: i for i, j in list(id2words.items())}\n",
    "print(f'words2id len = {len(words2id)}')\n",
    "\n",
    "# 保存word2id\n",
    "import pickle\n",
    "\n",
    "# 保存词表\n",
    "with open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl','wb') as f:\n",
    "    pickle.dump(words2id,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA03F5D94B07499EB14AAE66B1B596B8"
   },
   "outputs": [],
   "source": [
    "# 以下的cell使用训练好的词向量和使用的词表 制作神经网络需要的词向量矩阵 使用100维的word2vec和fasttext拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "C39DE450050349B790E46101C66568EC"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_file = '/home/kesci/work/word2vec/word2vec_100.bin' # word2vec\n",
    "fast_text_file = './fasttext/fasttext_100.bin'  # fasttext\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "emb_list = []\n",
    "w2v_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "fasttext_model = KeyedVectors.load_word2vec_format(fast_text_file, binary=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_words = len(words2id) + 2\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "from tqdm import tqdm\n",
    "for word, i in tqdm(words2id.items()):  # 因为训练的词向量没用过滤低频词 所以都可以命中\n",
    "    w2v = w2v_model[word]\n",
    "    fasttext = fasttext_model[word]\n",
    "    embedding_matrix[i] = np.concatenate([w2v, fasttext])\n",
    "\n",
    "np.save('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF08B6D62F9F4566880C89E4E1E9EA75"
   },
   "outputs": [],
   "source": [
    "# 以下是带有特征的esim网络的训练 前1e数据的 隐层个数为128 我们选择了前2轮的模型用于模型融合\n",
    "# 下面几个ceil都是相同的代码 只不过读取的是不同的数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "id": "BCD23047DA8849B287385C8227E3DBE3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Embedding, Dense, Dropout, Lambda, concatenate, GlobalAveragePooling1D, subtract, multiply, \\\n",
    "    TimeDistributed, LSTM\n",
    "from keras.regularizers import l2\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "seed = 2019\n",
    "\n",
    "set_random_seed(seed)  # Tensorflow\n",
    "np.random.seed(seed)  # NumPy\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.engine import Layer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(level=logging.INFO)\n",
    "handler = logging.FileHandler(\"./sunrui/gated_esim_64_pre_1e_64_log.txt\")  # 训练前1e的数据\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, return_attend_weight=False, keep_mask=True, **kwargs):\n",
    "        self.return_attend_weight = return_attend_weight\n",
    "        self.keep_mask = keep_mask\n",
    "        self.supports_masking = True\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        input_shape_a, input_shape_b = input_shape\n",
    "\n",
    "        if len(input_shape_a) != 3 or len(input_shape_b) != 3:\n",
    "            raise ValueError('Inputs into DotProductAttention should be 3D tensors')\n",
    "\n",
    "        if input_shape_a[-1] != input_shape_b[-1]:\n",
    "            raise ValueError('Inputs into DotProductAttention should have the same dimensionality at the last axis')\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        assert isinstance(inputs, list)\n",
    "        inputs_a, inputs_b = inputs\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_a, mask_b = mask\n",
    "        else:\n",
    "            mask_a, mask_b = None, None\n",
    "\n",
    "        e = K.exp(K.batch_dot(inputs_a, inputs_b, axes=2))  # similarity between a & b\n",
    "\n",
    "        if mask_a is not None:\n",
    "            e *= K.expand_dims(K.cast(mask_a, K.floatx()), 2)\n",
    "        if mask_b is not None:\n",
    "            e *= K.expand_dims(K.cast(mask_b, K.floatx()), 1)\n",
    "\n",
    "        e_b = e / K.cast(K.sum(e, axis=2, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over b\n",
    "        e_a = e / K.cast(K.sum(e, axis=1, keepdims=True) + K.epsilon(), K.floatx())  # attention weight over a\n",
    "\n",
    "        if self.return_attend_weight:\n",
    "            return [e_b, e_a]\n",
    "\n",
    "        a_attend = K.batch_dot(e_b, inputs_b, axes=(2, 1))  # a attend to b\n",
    "        b_attend = K.batch_dot(e_a, inputs_a, axes=(1, 1))  # b attend to a\n",
    "        return [a_attend, b_attend]\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if self.keep_mask:\n",
    "            return mask\n",
    "        else:\n",
    "            return [None, None]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attend_weight:\n",
    "            input_shape_a, input_shape_b = input_shape\n",
    "            return [(input_shape_a[0], input_shape_a[1], input_shape_b[1]),\n",
    "                    (input_shape_a[0], input_shape_a[1], input_shape_b[1])]\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class SWA(Callback):\n",
    "    def __init__(self, checkpoint_dir, model_name, swa_start=1):\n",
    "        super(SWA, self).__init__()\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.model_name = model_name\n",
    "        self.swa_start = swa_start\n",
    "        self.swa_model = None  # the model that we will use to store the average of the weights once SWA begins\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = 0\n",
    "        self.swa_n = 0\n",
    "        # self.swa_model = copy.deepcopy(self.model)  # make a copy of the model we're training\n",
    "        # Note: I found deep copy of a model with customized layer would give errors\n",
    "        self.swa_model = keras.models.clone_model(self.model)\n",
    "        self.swa_model.set_weights(self.model.get_weights())  # see: https://github.com/keras-team/keras/issues/1765\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (self.epoch + 1) >= self.swa_start:\n",
    "            self.update_average_model()\n",
    "            self.swa_n += 1\n",
    "\n",
    "        self.epoch += 1\n",
    "\n",
    "    def update_average_model(self):\n",
    "        # update running average of parameters\n",
    "        alpha = 1. / (self.swa_n + 1)\n",
    "        for layer, swa_layer in zip(self.model.layers, self.swa_model.layers):\n",
    "            weights = []\n",
    "            for w1, w2 in zip(swa_layer.get_weights(), layer.get_weights()):\n",
    "                weights.append((1 - alpha) * w1 + alpha * w2)\n",
    "            swa_layer.set_weights(weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print('Logging Info - Saving SWA model checkpoint: %s_swa.hdf5\\n' % self.model_name)\n",
    "        self.swa_model.save_weights(os.path.join(self.checkpoint_dir, '{}_swa.hdf5'.format(self.model_name)))\n",
    "        print('Logging Info - SWA model Saved')\n",
    "\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_lr=0.001,\n",
    "            max_lr=0.006,\n",
    "            step_size=2000.,\n",
    "            mode='triangular',\n",
    "            gamma=1.,\n",
    "            scale_fn=None,\n",
    "            scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2',\n",
    "                        'exp_range']:\n",
    "            raise KeyError(\"mode must be one of 'triangular', \"\n",
    "                           \"'triangular2', or 'exp_range'\")\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** x\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr is not None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr is not None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size is not None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                   np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                   np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault(\n",
    "            'lr', []).append(\n",
    "            K.get_value(\n",
    "                self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- 构建模型 --------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_feature_inputs(dense_features_: list):\n",
    "    dense_input = OrderedDict()\n",
    "    for feature in dense_features_:\n",
    "        dense_input[feature] = Input(shape=(1,), name=feature + '_input')\n",
    "    return dense_input\n",
    "\n",
    "\n",
    "def get_dense_feature_fc_list(dense_input_: OrderedDict, fc_dim=8, use_bias=True, l2_reg=1e-4):\n",
    "    dense_input = list(dense_input_.values())\n",
    "    fc_out_list = list(map(Dense(fc_dim, use_bias=use_bias, kernel_regularizer=l2(l2_reg)), dense_input))\n",
    "    return fc_out_list\n",
    "\n",
    "\n",
    "def build_model(lstm_dim=64, emb_mat=None):\n",
    "    print('Build model...')\n",
    "\n",
    "    query_input = Input(shape=(max_seq_len,))\n",
    "    title_input = Input(shape=(max_seq_len,))\n",
    "\n",
    "    embedding = Embedding(emb_mat.shape[0], W2V_DIM, weights=[emb_mat], trainable=False, mask_zero=True)\n",
    "\n",
    "    query_emb = embedding(query_input)\n",
    "    query_emb = Dropout(0.2)(query_emb)\n",
    "\n",
    "    title_emb = embedding(title_input)\n",
    "    title_emb = Dropout(0.2)(title_emb)\n",
    "\n",
    "    bilstm_1 = LSTM(units=lstm_dim, return_sequences=True)\n",
    "\n",
    "    query_hidden = bilstm_1(query_emb)\n",
    "    title_hidden = bilstm_1(title_emb)\n",
    "\n",
    "    query_attend, title_attend = DotProductAttention()([query_hidden, title_hidden])\n",
    "\n",
    "    query_enhance = concatenate([query_hidden, query_attend, subtract([query_hidden, query_attend]),\n",
    "                                 multiply([query_hidden, query_attend])])  # [?,25,256]\n",
    "\n",
    "    title_enhance = concatenate([title_hidden, title_attend,\n",
    "                                 subtract([title_hidden, title_attend]),\n",
    "                                 multiply([title_hidden, title_attend])])  # [?,25,256]\n",
    "\n",
    "    # inference composition\n",
    "    feed_forward = TimeDistributed(Dense(units=lstm_dim, activation='relu'))\n",
    "\n",
    "    bilstm_2 = LSTM(units=lstm_dim, return_sequences=True)\n",
    "\n",
    "    query_compose = bilstm_2(feed_forward(query_enhance))  # [?,25,32]\n",
    "    title_compose = bilstm_2(feed_forward(title_enhance))\n",
    "\n",
    "    global_max_pooling = Lambda(lambda x: K.max(x, axis=1))  # GlobalMaxPooling1D didn't support masking\n",
    "    query_avg = GlobalAveragePooling1D()(query_compose)\n",
    "    query_max = global_max_pooling(query_compose)\n",
    "    title_avg = GlobalAveragePooling1D()(title_compose)\n",
    "    title_max = global_max_pooling(title_compose)\n",
    "\n",
    "    lgb_dense_feature_input = get_dense_feature_inputs(used_lgb_dense_feature)\n",
    "\n",
    "    dense_fc_list = get_dense_feature_fc_list(lgb_dense_feature_input)\n",
    "\n",
    "    if len(dense_fc_list) > 1:\n",
    "        dense_feature_concat = concatenate(dense_fc_list)\n",
    "    else:\n",
    "        dense_feature_concat = dense_fc_list[0]\n",
    "\n",
    "    inference_compose = concatenate([query_avg, query_max, title_avg, title_max])\n",
    "\n",
    "    # inference_compose = BatchNormalization()(inference_compose)  # 尝试\n",
    "\n",
    "    dense_esim = Dense(units=lstm_dim)(inference_compose)\n",
    "\n",
    "    dense_feature_gate = Dense(lstm_dim, activation='sigmoid')(dense_feature_concat)\n",
    "\n",
    "    gated_esim = Lambda(lambda x: x[0] * x[1])([dense_esim, dense_feature_gate])\n",
    "\n",
    "    gated_esim = Dense(lstm_dim, activation='elu')(gated_esim)\n",
    "    model_dense_input = [lgb_dense_feature_input[feat] for feat in used_lgb_dense_feature]\n",
    "\n",
    "    # gated_esim = BatchNormalization()(gated_esim)  # 加了BN 训练不稳定 val loss会跳 但是性能尚可\n",
    "    # gated_esim = Dropout(0.1)(gated_esim)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(gated_esim)\n",
    "    model = Model(inputs=[query_input, title_input] + model_dense_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评价指标 并行计算QAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def cal_AUC(labels, prob):\n",
    "    try:\n",
    "        return roc_auc_score(labels, prob)\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "\n",
    "# 计算各个组的qauc值\n",
    "def sum_AUC(mycombinedata):\n",
    "    grouplist = mycombinedata[0]\n",
    "    y_true = mycombinedata[1]\n",
    "    y_pred = mycombinedata[2]\n",
    "\n",
    "    if len(y_true) != sum(grouplist):\n",
    "        print(\"评分函数中len(y_true)!=sum(group)\")\n",
    "        return\n",
    "    start = 0\n",
    "    sum_AUC = 0\n",
    "    for group in grouplist:\n",
    "        roc_auc = cal_AUC(y_true[start:start + group], y_pred[start:start + group])\n",
    "        start = start + group\n",
    "        sum_AUC = sum_AUC + roc_auc\n",
    "    return sum_AUC\n",
    "\n",
    "\n",
    "def QAUC_parallel(y_true, y_pred, group):\n",
    "    groupnum = 4\n",
    "    import math\n",
    "    group_len = math.ceil(len(group) / groupnum)\n",
    "    groups = [group[i * group_len:(i + 1) * group_len] for i in range(groupnum)]\n",
    "    mycombines = []\n",
    "\n",
    "    start = 0\n",
    "    for agroup in groups:\n",
    "        mycombinedata = []\n",
    "        mycombinedata.append(agroup)\n",
    "        mycombinedata.append(y_true[start:start + sum(agroup)])\n",
    "        mycombinedata.append(y_pred[start:start + sum(agroup)])\n",
    "        start = start + sum(agroup)\n",
    "        mycombines.append(mycombinedata)\n",
    "\n",
    "    sum_AUC_ = Parallel(n_jobs=groupnum)(delayed(sum_AUC)(mycombinedata) for mycombinedata in mycombines)\n",
    "\n",
    "    return sum(sum_AUC_) / len(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据及模型训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.x_val, self.val_group, self.y_val = validation_data\n",
    "        self.best_score = 0.\n",
    "        self.best_epoch = 0\n",
    "        self.best_auc = 0.0\n",
    "        self.best_auc_epoch = 0\n",
    "        self.auc_list = []\n",
    "        self.q_auc_list = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0, batch_size=batch_size)\n",
    "            auc_score = roc_auc_score(self.y_val, y_pred)\n",
    "            self.auc_list.append(auc_score)\n",
    "            if auc_score > self.best_auc:\n",
    "                self.best_auc = auc_score\n",
    "                self.best_auc_epoch = epoch + 1\n",
    "\n",
    "            score_parallel = QAUC_parallel(self.y_val, y_pred, self.val_group)\n",
    "            self.q_auc_list.append(score_parallel)\n",
    "            if score_parallel > self.best_score:\n",
    "                self.best_score = score_parallel\n",
    "                self.best_epoch = epoch + 1\n",
    "            logger.info(f'Q_AUC = {score_parallel} epoch = {epoch + 1}')\n",
    "\n",
    "            print('\\n ROC_AUC - epoch:%d - score:%.6f' % (epoch + 1, auc_score))\n",
    "            print('\\n Q_AUC - epoch:%d - score:%.6f' % (epoch + 1, score_parallel))\n",
    "\n",
    "    def get_best_score_epoch(self):\n",
    "        return self.best_score, self.best_epoch\n",
    "\n",
    "    def get_best_auc_score_epoch(self):\n",
    "        return self.best_auc, self.best_auc_epoch\n",
    "\n",
    "    def show_result_list(self):\n",
    "        print('auc', self.auc_list)\n",
    "        print('\\n')\n",
    "        print('qauc', self.q_auc_list)\n",
    "\n",
    "\n",
    "def seq_padding(X, max_len):\n",
    "    return [x + [PAD] * (max_len - len(x)) if len(x) < max_len else x[:max_len] for x in X]\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, word2id, text_data, lgb_data, batch_size=1024 * 5):\n",
    "        self.batch_size = batch_size\n",
    "        self.word2id = word2id\n",
    "\n",
    "        self.text_data = text_data\n",
    "        self.lgb_data = lgb_data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 计算每一个epoch的迭代次数\n",
    "        return math.ceil(len(self.text_data) / float(self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        stop = (index + 1) * self.batch_size\n",
    "\n",
    "        batch_lgb_df = self.lgb_data.iloc[start:stop]\n",
    "        batch_text_df = self.text_data.iloc[start:stop]\n",
    "        y = batch_text_df['label'].values\n",
    "\n",
    "        train_lgb_input = [batch_lgb_df[feat].values for feat in used_lgb_dense_feature]\n",
    "\n",
    "        Q = []\n",
    "        D = []\n",
    "        for query in batch_text_df['query']:\n",
    "            query = query.split()\n",
    "            Q.append([word2id[w] for w in query])\n",
    "\n",
    "        for title in batch_text_df['title']:\n",
    "            title = title.split()\n",
    "            D.append([word2id[w] for w in title])\n",
    "\n",
    "        Q_pad = seq_padding(Q, max_seq_len)\n",
    "        D_pad = seq_padding(D, max_seq_len)\n",
    "        return [np.array(Q_pad), np.array(D_pad)] + train_lgb_input, y\n",
    "\n",
    "\n",
    "flag = 'train'\n",
    "batch_size = 1024 * 5\n",
    "\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "\n",
    "W2V_DIM = 200\n",
    "\n",
    "max_seq_len = 25\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "def get_used_feature_names(featurecol_h5):\n",
    "    features = []\n",
    "    for k, v in featurecol_h5.items():\n",
    "        features.extend(v)\n",
    "    return features\n",
    "\n",
    "\n",
    "featurecol_h5 = {\n",
    "    'sim_feat': ['jaccard_q3_t3',\n",
    "                 'jaccard_q3_t5',\n",
    "                 'jaccard_q5_t5', 'levenshtein_q5_t5',\n",
    "                 'jaccard_q5_t10', 'levenshtein_q5_t10',\n",
    "                 'jaccard_q10_t10', 'levenshtein_q10_t10',\n",
    "                 'jaccard_q15_t25', 'levenshtein_q15_t25',\n",
    "                 'jaccard', 'levenshtein'],\n",
    "\n",
    "    'len_feat': [\"querykw_num\", \"titlekw_num\"],\n",
    "\n",
    "    \"title_nunique_query\": [\"title_nunique_query\"],\n",
    "    \"query_nunique_title\": [\"query_nunique_title\"],\n",
    "\n",
    "    'title_score_count_feat': [\"title_score_count\", \"title_score_click_num\"],\n",
    "    'title_code_score_feat': [\"title_code_score\"],\n",
    "    'title_convert_feat': [\"title_code_convert\", 'title_code_label_count'],\n",
    "\n",
    "    'query_count': [\"query_code_count\"],\n",
    "    'title_count': [\"title_count\"],\n",
    "\n",
    "    \"match_feat\": ['count_match', 'blockcount_match', 'proximity', 'maxMatchBlockLen',\n",
    "                   'q1_match_start', 'q1_match_end'],\n",
    "\n",
    "    \"BM25\": [\"BM25\"],\n",
    "}\n",
    "\n",
    "othercols = [\"titlekw_querykw_diff\", \"titlekw_querykw_rate\"]\n",
    "\n",
    "\n",
    "def reduce_mem_usage(D, verbose=True):\n",
    "    start_mem = D.memory_usage().sum() / 1024 ** 2\n",
    "    for c, d in zip(D.columns, D.dtypes):\n",
    "        if d.kind == 'f':\n",
    "            D[c] = pd.to_numeric(D[c], downcast='float')\n",
    "        elif d.kind == 'i':\n",
    "            D[c] = pd.to_numeric(D[c], downcast='signed')\n",
    "    end_mem = D.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
    "            start_mem - end_mem) / start_mem))\n",
    "    return D\n",
    "\n",
    "\n",
    "def ReadData(datatype='train', nrows=1000000000):\n",
    "    if datatype == 'train':\n",
    "        id_feature = '/home/kesci/input/bytedance/train_final.csv'\n",
    "        usecols = [0, 4]\n",
    "        names = ['query_id', 'label']\n",
    "\n",
    "        print(\"read \", id_feature)\n",
    "        DataSet = pd.read_csv(id_feature,\n",
    "                              header=None,\n",
    "                              nrows=nrows,\n",
    "                              usecols=usecols,\n",
    "                              names=names\n",
    "                              )\n",
    "        path_h5 = \"/home/kesci/work/pre_3billion_data/train/\"\n",
    "    elif datatype == 'test1':\n",
    "        id_feature = '/home/kesci/input/bytedance/test_final_part1.csv'\n",
    "        usecols = [0, 2]\n",
    "        names = ['query_id', 'query_title_id']\n",
    "        path_h5 = \"/home/kesci/work/post_4kw_data/test1/\"\n",
    "        print(\"read \", id_feature)\n",
    "        DataSet = pd.read_csv(id_feature,\n",
    "                              header=None,\n",
    "                              nrows=nrows,\n",
    "                              usecols=usecols,\n",
    "                              names=names\n",
    "                              )\n",
    "    elif datatype == 'test2':\n",
    "        id_feature = '/home/kesci/input/bytedance/bytedance_contest.final_2.csv'\n",
    "        usecols = [0, 2]\n",
    "        names = ['query_id', 'query_title_id']\n",
    "        path_h5 = \"/home/kesci/work/post_4kw_data/test2/\"\n",
    "        print(\"read \", id_feature)\n",
    "        DataSet = pd.read_csv(id_feature,\n",
    "                              header=None,\n",
    "                              nrows=nrows,\n",
    "                              usecols=usecols,\n",
    "                              names=names\n",
    "                              )\n",
    "\n",
    "    print(\"length:\", DataSet.__len__())\n",
    "    DataSet = reduce_mem_usage(DataSet, verbose=True)\n",
    "\n",
    "    featuremap_h5 = {\n",
    "        'cross_feat': path_h5 + f'cross_{datatype}_feat.h5',\n",
    "\n",
    "        'query_pos_feat': path_h5 + f'query_pos_{datatype}_feat.h5',\n",
    "        'title_pos_feat': path_h5 + f'title_pos_{datatype}_feat.h5',\n",
    "\n",
    "        'match_feat': path_h5 + f'query_match_{datatype}_feat.h5',\n",
    "        'editDistance_feat': path_h5 + f'editDistance_{datatype}_feat.h5',\n",
    "\n",
    "        'sim_feat': path_h5 + f'sim_{datatype}_feat.h5',\n",
    "        'tag_score_feat': path_h5 + f'tag_score_10foldtime_{datatype}_feat.h5',\n",
    "        'title_score_count_feat': path_h5 + f'title_score_count_{datatype}_feat.h5',\n",
    "        'title_code_score_feat': path_h5 + f'title_code_score_10foldtime_{datatype}_feat.h5',\n",
    "        'title_convert_feat': path_h5 + f'title_convert_{datatype}.h5',\n",
    "        'sif_feat': path_h5 + f'sif_{datatype}_post_4kw.h5',\n",
    "        'len_feat': path_h5 + f'len_{datatype}_feat.h5',\n",
    "\n",
    "        'title_count': path_h5 + f'count_feature_{datatype}.h5',\n",
    "        'query_count': path_h5 + f'query_count_all_{datatype}.h5',\n",
    "\n",
    "        \"title_nunique_query\": path_h5 + f'nunique_feature_{datatype}.h5',\n",
    "        \"query_nunique_title\": path_h5 + f'query_nunique_title_all_{datatype}.h5',\n",
    "\n",
    "        'tag': path_h5 + f'tag_{datatype}.h5',\n",
    "        \"tag_convert_feat\": path_h5 + f\"tag_convert_{datatype}.h5\",\n",
    "        \"query_convert\": path_h5 + f\"query_convert_{datatype}.h5\",\n",
    "\n",
    "        \"M_cosine\": path_h5 + f\"M_sim_{datatype}_feat.h5\",\n",
    "        \"M_tfidf_cosine\": path_h5 + f\"M_tfidf_sim_{datatype}_feat.h5\",\n",
    "        \"BM25\": path_h5 + f'BM25_{datatype}_feat.h5',\n",
    "        'NN_SIM': path_h5 + f'nn_sim_feature.h5',\n",
    "\n",
    "        'editdistance_relativepos': path_h5 + f'editdistance_relativepos_{datatype}_feat.h5',\n",
    "        'fuzz': path_h5 + f\"fuzz_{datatype}_feat.h5\",\n",
    "        'textpair': path_h5 + f\"textpair_{datatype}_feat.h5\",\n",
    "\n",
    "        'sen_dis': path_h5 + f\"sen_dis_{datatype}_200.h5\",\n",
    "        'sen_dis2': path_h5 + f\"sen_dis2_{datatype}_200.h5\",\n",
    "    }\n",
    "\n",
    "    for featurefile in featurecol_h5:\n",
    "        print(\"read \", featuremap_h5[featurefile])\n",
    "        feature_set = pd.read_hdf(featuremap_h5[featurefile],\n",
    "                                  key='data',\n",
    "                                  start=0,\n",
    "                                  stop=nrows)[featurecol_h5[featurefile]].reset_index(drop=True)\n",
    "        print(\"length:\", feature_set.__len__())\n",
    "        # print(feature_set.head(1))\n",
    "        # feature_set=reduce_mem_usage(feature_set, verbose=True)\n",
    "        DataSet = pd.concat([DataSet, feature_set], axis=1)\n",
    "\n",
    "    DataSet[\"titlekw_querykw_diff\"] = DataSet[\"titlekw_num\"] - DataSet[\"querykw_num\"]\n",
    "    DataSet[\"titlekw_querykw_rate\"] = DataSet[\"titlekw_num\"] / DataSet[\"querykw_num\"]\n",
    "\n",
    "    if \"title_code_score\" in DataSet.columns:\n",
    "        DataSet.title_code_score = DataSet.title_code_score.fillna(0)\n",
    "    if \"tag_score\" in DataSet.columns:\n",
    "        DataSet.tag_score = DataSet.tag_score.fillna(0)\n",
    "\n",
    "    DataSet = reduce_mem_usage(DataSet, verbose=True)\n",
    "    print(\"Data Read Finish!\")\n",
    "    return DataSet\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if flag == 'train':\n",
    "\n",
    "        train_size = 98000000\n",
    "        emb_mat = np.load('/home/kesci/work/sunrui/NN_second_2e/word2vec_fasttext_6kw_nn_sim.npy')\n",
    "        \n",
    "        lgb_data = ReadData(datatype='train', nrows=100000000)\n",
    "        used_lgb_dense_feature = get_used_feature_names(featurecol_h5) + othercols\n",
    "\n",
    "        print(used_lgb_dense_feature)\n",
    "\n",
    "        text_data = pd.read_csv('/home/kesci/input/bytedance/train_final.csv', usecols=[0, 1, 3, 4], header=None,\n",
    "            names=['query_id', 'query', 'title', 'label'], nrows=100000000)\n",
    "        # 读取 lgb feature\n",
    "        print(text_data.shape)\n",
    "        lgb_data[used_lgb_dense_feature] = lgb_data[used_lgb_dense_feature].fillna(-1, )\n",
    "\n",
    "        train_lgb_data = lgb_data[:train_size]\n",
    "        val_lgb_data = lgb_data[train_size:]\n",
    "        # 读取 lgb feature 完毕\n",
    "\n",
    "        with open('/home/kesci/work/sunrui/NN_second_2e/second_6kw_nn_sim.pkl', 'rb') as f:\n",
    "            word2id = pickle.load(f)\n",
    "\n",
    "        val_text_data = text_data[train_size:]\n",
    "        train_text_data = text_data[:train_size]\n",
    "\n",
    "        Q_val = []\n",
    "        D_val = []\n",
    "        for query in val_text_data['query']:\n",
    "            query = query.split()\n",
    "            Q_val.append([word2id[w] for w in query])  # 没有命中就返回UNK\n",
    "\n",
    "        for title in val_text_data['title']:\n",
    "            title = title.split()\n",
    "            D_val.append([word2id[w] for w in title])\n",
    "\n",
    "        val_query_input = seq_padding(Q_val, max_seq_len)\n",
    "        val_title_input = seq_padding(D_val, max_seq_len)\n",
    "\n",
    "        Y_val = val_text_data['label'].values\n",
    "\n",
    "        train_generator = DataGenerator(word2id=word2id, text_data=train_text_data,\n",
    "                                        lgb_data=train_lgb_data, batch_size=batch_size)\n",
    "\n",
    "        val_text_data['query_id_nums'] = val_text_data.groupby(['query_id'])['label'].transform('count')\n",
    "\n",
    "        val_group_df = val_text_data[['query_id', 'query_id_nums']].drop_duplicates()\n",
    "        val_group = val_group_df.query_id_nums.get_values()\n",
    "\n",
    "        swa = SWA(checkpoint_dir='./sunrui/swa/', model_name='swa.model')\n",
    "\n",
    "        clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                       step_size=2 * train_size // batch_size, mode='triangular')\n",
    "\n",
    "        model = build_model(lstm_dim=128, emb_mat=emb_mat)\n",
    "\n",
    "        filepath = \"/home/kesci/work/sunrui/nn/gated_pre_1e/128dim/gated-{epoch:02d}_esim_128_pre1e.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, verbose=1)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_auc', patience=5, verbose=1, mode='max')\n",
    "\n",
    "        val_lgb_input = [val_lgb_data[feat].values for feat in used_lgb_dense_feature]\n",
    "\n",
    "        # train_model_input = [train_query_input, train_title_input] + train_lgb_input\n",
    "        val_model_input = [val_query_input, val_title_input] + val_lgb_input\n",
    "\n",
    "        eval_callback = Evaluation(\n",
    "            validation_data=(\n",
    "                val_model_input, val_group, Y_val))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "        model.summary()\n",
    "        model.fit_generator(train_generator, epochs=epochs,\n",
    "                            validation_data=(val_model_input, Y_val),\n",
    "                            callbacks=[early_stopping, eval_callback, swa, clr, checkpoint], shuffle=True,\n",
    "                            workers=2,\n",
    "                            use_multiprocessing=True)\n",
    "    else:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
